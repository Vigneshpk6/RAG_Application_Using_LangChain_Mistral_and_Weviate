# RAG_Application_Using_LangChain_Mistral_and_Weviate

ğŸ§  RAG Application Using LangChain + Mistral + Weaviate

This project is a Retrieval-Augmented Generation (RAG) application built using:

LangChain

Mistral LLM

Weaviate Vector Database

It allows you to store documents inside Weaviate, retrieve relevant chunks using semantic search, and generate accurate answers using Mistral.

ğŸš€ Features

âœ… Load and process documents
âœ… Split text into chunks
âœ… Create embeddings and store into Weaviate
âœ… Semantic retrieval (Top-K chunks)
âœ… Generate final response using Mistral LLM
âœ… End-to-end RAG pipeline

ğŸ› ï¸ Tech Stack

Python

LangChain

Mistral API

Weaviate Cloud / Local Weaviate

Embeddings (HuggingFace / OpenAI)

ğŸ“‚ Project Files
RAG_Application_Using_LangChain_Mistral_and_Weviate.ipynb â†’ Main notebook (complete implementation)

rag document.pdf â†’ Reference paper / explanation of RAG concept
